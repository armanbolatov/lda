{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-Class Classification on Synthetic Data with an LDA Head\n",
    "This notebook builds a toy 2D dataset with three Gaussian clusters, trains a small encoder paired with an LDA head, and visualises the resulting decision regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from src.lda import LDAHeadMLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33cbb9",
   "metadata": {},
   "source": [
    "### Generate Synthetic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fe6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Gaussian clusters in 2D for 3-way classification.\n",
    "means = [\n",
    "    torch.tensor([-2.0, 0.0]),\n",
    "    torch.tensor([2.0, 0.5]),\n",
    "    torch.tensor([0.5, 3.0])\n",
    "]\n",
    "covariances = [\n",
    "    torch.tensor([[1.0, 0.2], [0.2, 1.2]]),\n",
    "    torch.tensor([[1.3, -0.3], [-0.3, 1.0]]),\n",
    "    torch.tensor([[0.8, 0.1], [0.1, 1.1]])\n",
    "]\n",
    "generators = [\n",
    "    torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "    for mean, cov in zip(means, covariances)\n",
    "]\n",
    "NUM_CLASSES = len(generators)\n",
    "\n",
    "def sample_split(n_total):\n",
    "    base = n_total // NUM_CLASSES\n",
    "    remainder = n_total % NUM_CLASSES\n",
    "    counts = [base + (1 if i < remainder else 0) for i in range(NUM_CLASSES)]\n",
    "\n",
    "    xs, ys = [], []\n",
    "    for cls, (gen, n_cls) in enumerate(zip(generators, counts)):\n",
    "        xs.append(gen.sample((n_cls,)))\n",
    "        ys.append(torch.full((n_cls,), cls, dtype=torch.long))\n",
    "\n",
    "    x = torch.cat(xs, dim=0)\n",
    "    y = torch.cat(ys, dim=0)\n",
    "    idx = torch.randperm(x.size(0))\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "train_X, train_y = sample_split(20_000)\n",
    "test_X, test_y = sample_split(4_000)\n",
    "\n",
    "train_ds = TensorDataset(train_X, train_y)\n",
    "test_ds = TensorDataset(test_X, test_y)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_ld = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
    "\n",
    "train_X[:5], train_y[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f9e71b",
   "metadata": {},
   "source": [
    "### Model: small encoder + LDA head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 32), nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepLDA(nn.Module):\n",
    "    def __init__(self, C, D):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(D)\n",
    "        self.head = LDAHeadMLE(C, D)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        z = self.encoder(x)\n",
    "        return self.head(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb28f6d",
   "metadata": {},
   "source": [
    "### Training Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c466bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ok = tot = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        ok += (logits.argmax(dim=1) == y).sum().item()\n",
    "        tot += y.size(0)\n",
    "    return ok / tot\n",
    "\n",
    "@torch.no_grad()\n",
    "def nll_on_loader(model, loader, device):\n",
    "    model.eval()\n",
    "    loss_sum = tot = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        z = model.encoder(X)\n",
    "        batch_loss = model.head.joint_nll(z, y)\n",
    "        loss_sum += batch_loss.item() * y.size(0)\n",
    "        tot += y.size(0)\n",
    "    return loss_sum / tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3f664",
   "metadata": {},
   "source": [
    "### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device =', device)\n",
    "\n",
    "model = DeepLDA(C=NUM_CLASSES, D=2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    model.train()\n",
    "    loss_sum = tot = correct = 0\n",
    "    for X, y in train_ld:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        z = model.encoder(X)\n",
    "        loss = model.head.joint_nll(z, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model.head(z)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            tot += y.size(0)\n",
    "            loss_sum += loss.item() * y.size(0)\n",
    "\n",
    "    train_acc = correct / tot\n",
    "    test_acc = evaluate(model, test_ld, device)\n",
    "    test_nll = nll_on_loader(model, test_ld, device)\n",
    "    print(f\"[epoch {epoch:02d}] train loss={loss_sum/tot:.4f} acc={train_acc:.4f} | test acc={test_acc:.4f} nll={test_nll:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403139c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829948d",
   "metadata": {},
   "source": [
    "### Collect a Small Random Subset of Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list, y_list = [], []\n",
    "max_points = int(len(train_ld.dataset) * 0.05)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in train_ld:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        z = model.encoder(X)\n",
    "        emb_list.append(z)\n",
    "        y_list.append(y)\n",
    "        if sum(t.shape[0] for t in emb_list) >= max_points:\n",
    "            break\n",
    "\n",
    "Z = torch.cat(emb_list, dim=0).cpu().numpy()[:max_points]\n",
    "Y = torch.cat(y_list, dim=0).cpu().numpy()[:max_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9743df5",
   "metadata": {},
   "source": [
    "### Inspect Learned Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = model.head.mu.detach().cpu()  # (C, K, D) or (C*K, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35c796",
   "metadata": {},
   "source": [
    "### Visualise Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "OKABE_ITO = [\"#0072B2\", \"#D55E00\", \"#009E73\", \"#CC79A7\", \"#56B4E9\",\n",
    "             \"#E69F00\", \"#F0E442\", \"#000000\", \"#999999\", \"#E66100\"]\n",
    "markers = [\"o\", \"s\", \"^\", \"v\", \"D\", \"P\", \"X\", \"*\", \"h\", \">\"]\n",
    "\n",
    "class_means = mu if mu.dim() == 2 else mu.mean(dim=1)\n",
    "centers = class_means.numpy()\n",
    "color_cycle = (OKABE_ITO * ((centers.shape[0] // len(OKABE_ITO)) + 1))[:centers.shape[0]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    L = model.head._chol_factor().detach().cpu().numpy()\n",
    "Sigma = L @ L.T\n",
    "\n",
    "def cov_ellipse(mean, cov, k=2.15, **kwargs):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    vals, vecs = vals[order], vecs[:, order]\n",
    "    width, height = 2 * k * np.sqrt(np.maximum(vals, 1e-12))\n",
    "    angle = np.degrees(np.arctan2(vecs[1, 0], vecs[0, 0]))\n",
    "    return Ellipse(xy=mean, width=width, height=height, angle=angle, **kwargs)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 13,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(7.2, 5.8), constrained_layout=True)\n",
    "\n",
    "for c in range(centers.shape[0]):\n",
    "    idx = (Y == c)\n",
    "    marker = markers[c % len(markers)]\n",
    "    ax.scatter(Z[idx, 0], Z[idx, 1], s=32, c=color_cycle[c], marker=marker,\n",
    "               edgecolors=\"black\", linewidths=0.55, alpha=0.9, label=f\"class {c+1}\")\n",
    "    m = centers[c]\n",
    "    ax.scatter(m[0], m[1], s=150, marker=\"X\", c=color_cycle[c],\n",
    "               edgecolors=\"black\", linewidths=1.0, zorder=5)\n",
    "    ellipse = cov_ellipse(m, Sigma, k=2.15, edgecolor=\"black\",\n",
    "                          facecolor=\"none\", lw=1.1, linestyle=\"--\", alpha=0.85)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "ax.set_title(\"Deep LDA on Synthetic Data (MLE head)\")\n",
    "ax.set_xlabel(r\"$z_1$\")\n",
    "ax.set_ylabel(r\"$z_2$\")\n",
    "ax.axis(\"equal\")\n",
    "pad = 0.8\n",
    "xmin, xmax = Z[:, 0].min() - pad, Z[:, 0].max() + pad\n",
    "ymin, ymax = Z[:, 1].min() - pad, Z[:, 1].max() + pad\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.legend(frameon=False, loc=\"upper right\", ncol=min(3, centers.shape[0]))\n",
    "ax.grid(True, linewidth=0.45, alpha=0.35, linestyle=\"--\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5828cde9504051a95539f11701fe4fe633331e3e80ce955880f142f23888e20c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}